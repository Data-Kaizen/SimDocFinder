{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aXHeMJyqCx3E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from functools import lru_cache\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy3\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5dUo148FjrP",
        "outputId": "5405640c-86c8-4107-8a46-5b912d536603"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4NcLWwPdKIR"
      },
      "source": [
        "Тестовый набор данных содержит информацию о проектах, поданных на конкурс. Первый столбец - номер, где первые две цифры обозначают год, а следующая цифра — номер конкурса. Например, номер 22-2-006088 означает 2022 год, 2 конкурс. Эта информация будет полезна при поиске похожих текстов.\n",
        "Второй столбец содержит текст с кратким описанием проекта и обоснованием его социальной значимости."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "EcjverlLBVxC",
        "outputId": "99b71051-4ca2-4656-fa81-79f18c6a81c4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"applications_dataset.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a4ms4vaICrJ7"
      },
      "outputs": [],
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=100000)\n",
        "def get_normal_form(word):\n",
        "    return morph.parse(word.lower())[0].normal_form\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Функция для очистки текста.\n",
        "    \"\"\"\n",
        "    # Удаление URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    # Удаление спецсимволов и чисел\n",
        "    text = re.sub(r\"[\\n%\\t•,\\\"'\\[\\]{}()«»1234567890№:;!]\", \"\", text)\n",
        "    # Заменяем дефисы на пробел\n",
        "    text = re.sub(r\"[-–]\", \" \", text)\n",
        "    # Заменяем точки на пробел\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    # Удаление лишних пробелов\n",
        "    text = re.sub(\n",
        "        r\"\\s+\", \" \", text\n",
        "    ).strip()  # Удаляем лишние пробелы и обрезаем пробелы по краям\n",
        "    return text\n",
        "\n",
        "\n",
        "def lemmatized_text(text):\n",
        "    \"\"\"\n",
        "    Функция лемматизирует исходный текст.\n",
        "    \"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    # Лемматизация слов с использованием кэша\n",
        "    lemmatized_words = [get_normal_form(word) for word in words]\n",
        "    lemmatized_text = \" \".join(lemmatized_words)\n",
        "    return lemmatized_text\n",
        "\n",
        "\n",
        "def del_stop_words(text):\n",
        "    \"\"\"\n",
        "    Функция для удаления стоп-слов.\n",
        "    \"\"\"\n",
        "    stop_words = stopwords.words(\"russian\")\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    filtered_text = \" \".join(filtered_words)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z_NBfm8iJ3oQ"
      },
      "outputs": [],
      "source": [
        "# Создание шинглов по словам\n",
        "def create_shingles(tokens, k=3):\n",
        "    \"\"\"\n",
        "    Функция создает шинглы.\n",
        "    \"\"\"\n",
        "    tokens = tokens.split()\n",
        "    shingl = [\" \".join(tokens[i : i + k]) for i in range(len(tokens) - k + 1)]\n",
        "    return shingl\n",
        "\n",
        "\n",
        "def create_minhash(tokens, num_perm=128, k=3):\n",
        "    \"\"\"\n",
        "    Функция создает MinHash.\n",
        "    \"\"\"\n",
        "    m = MinHash(num_perm=num_perm, seed=1631997)\n",
        "    for shingle in tokens:\n",
        "        m.update(shingle.encode(\"utf8\"))\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WLAutzRFWSqz"
      },
      "outputs": [],
      "source": [
        "def get_shingles_position(k, document):\n",
        "    \"\"\"\n",
        "    Находит положение шинглов в тексте.\n",
        "    \"\"\"\n",
        "    shingles_positions = {}\n",
        "    words = document.split()\n",
        "\n",
        "    for i in range(0, len(words) - k + 1):\n",
        "        shingle_words = words[i : i + k]\n",
        "        shingle = \" \".join(shingle_words)\n",
        "        if shingle in shingles_positions:\n",
        "            shingles_positions[shingle].append((i, i + k - 1))\n",
        "        else:\n",
        "            shingles_positions[shingle] = [(i, i + k - 1)]\n",
        "\n",
        "    return shingles_positions\n",
        "\n",
        "\n",
        "def merge_positions(positions):\n",
        "    \"\"\"\n",
        "    Объединяет позиции шинглов, если они перекрываются или соприкасаются.\n",
        "    \"\"\"\n",
        "    if not positions:\n",
        "        return []\n",
        "\n",
        "    # Собираем все позиции в один список\n",
        "    shingle_positions = []\n",
        "    for el in positions.values():\n",
        "        shingle_positions.extend(el)\n",
        "\n",
        "    # Сортируем позиции по началу\n",
        "    shingle_positions.sort()\n",
        "\n",
        "    # Объединяем позиции\n",
        "    merged = []\n",
        "    current_start, current_end = shingle_positions[0]\n",
        "\n",
        "    for start, end in shingle_positions[1:]:\n",
        "        if start <= current_end + 1:  # Если позиции перекрываются или соприкасаются\n",
        "            current_end = max(current_end, end)\n",
        "        else:\n",
        "            merged.append([current_start, current_end])  # Добавляем текущий интервал\n",
        "            current_start, current_end = start, end\n",
        "\n",
        "    # Добавляем последний интервал\n",
        "    merged.append([current_start, current_end])\n",
        "    return merged\n",
        "\n",
        "\n",
        "def search_shingles_position(ordinalnumber1, ordinalnumber2):\n",
        "    \"\"\"\n",
        "    Находит позиции совпадающих шинглов в двух текстах.\n",
        "    \"\"\"\n",
        "    # Получаем тексты\n",
        "    text_original = df[df[\"ordinalnumber\"] == ordinalnumber1][\"lemmatized_text\"].values[\n",
        "        0\n",
        "    ]\n",
        "    text_suspected = df[df[\"ordinalnumber\"] == ordinalnumber2][\n",
        "        \"lemmatized_text\"\n",
        "    ].values[0]\n",
        "\n",
        "    # Получаем позиции шинглов\n",
        "    shingles_positions_original = get_shingles_position(3, text_original)\n",
        "    shingles_positions_suspected = get_shingles_position(3, text_suspected)\n",
        "\n",
        "    # Находим совпадающие шинглы\n",
        "    plagiarized_shingles = {}\n",
        "    for shingle, positions in shingles_positions_suspected.items():\n",
        "        if shingle in shingles_positions_original:\n",
        "            plagiarized_shingles[shingle] = positions\n",
        "\n",
        "    # Объединяем позиции\n",
        "    merged = merge_positions(plagiarized_shingles)\n",
        "\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9ZXq89WdVKZW"
      },
      "outputs": [],
      "source": [
        "def calculate_lsh_similarity(df, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Cчитает LSH и находит потенциальные пары заявок, в которых есть заимствования.\n",
        "    \"\"\"\n",
        "    print(\"Start calculate Lsh..\")\n",
        "\n",
        "    # Создаем LSH объект\n",
        "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
        "\n",
        "    # Добавляем заявки и их minhash значения в LSH объект\n",
        "    for ordinalnumber, minhash in zip(df[\"ordinalnumber\"], df[\"minhash\"]):\n",
        "        lsh.insert(ordinalnumber, minhash)\n",
        "\n",
        "    print(\"LSH Similarity computed\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for original_ordinalnumber in tqdm(\n",
        "        df[\"ordinalnumber\"], desc=\"Processing LSH Queries\"\n",
        "    ):\n",
        "        # Ищем похожие заявки с помощью LSH\n",
        "        similar_items = lsh.query(\n",
        "            df[df[\"ordinalnumber\"] == original_ordinalnumber][\"minhash\"].values[0]\n",
        "        )\n",
        "        for similar_ordinalnumber in similar_items:\n",
        "            # Проверка, что оригинальный текст был добавлен раньше по времени, чем похожий текст\n",
        "            if original_ordinalnumber < similar_ordinalnumber:\n",
        "                results.append((original_ordinalnumber, similar_ordinalnumber))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Ut0XDbHVNhv"
      },
      "outputs": [],
      "source": [
        "def final_search_for_similar_requests(results, trash=40):\n",
        "    \"\"\"\n",
        "    Находит номера похожих заявок со сходством больше, чем trash и возвращает словарь похожих пар заявок и их процент сходства.\n",
        "    \"\"\"\n",
        "\n",
        "    results_application = []\n",
        "\n",
        "    for ordinalnumber1, ordinalnumber2 in tqdm(\n",
        "        results, desc=\"Processing Shingles Comparison\"\n",
        "    ):\n",
        "        # Получаем значения шинглов\n",
        "        shingles1 = set(df[df[\"ordinalnumber\"] == ordinalnumber1][\"shingles\"].values[0])\n",
        "        shingles2 = set(df[df[\"ordinalnumber\"] == ordinalnumber2][\"shingles\"].values[0])\n",
        "\n",
        "        # Находим пересечение шинглов\n",
        "        common_shingles = shingles1.intersection(shingles2)\n",
        "\n",
        "        # Рассчитываем процент совпадения по коэффициенту Сёренсена\n",
        "        percent_match = (\n",
        "            (2 * len(common_shingles)) / (len(shingles1) + len(shingles2)) * 100\n",
        "        )\n",
        "        if percent_match >= trash:\n",
        "            shingles_pos = search_shingles_position(ordinalnumber1, ordinalnumber2)\n",
        "            results_application.append(\n",
        "                {\n",
        "                    \"applications\": (ordinalnumber1, ordinalnumber2),\n",
        "                    \"similarity\": percent_match,\n",
        "                    \"shingles_positions\": shingles_pos,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return results_application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "zGK11aTfEWyH",
        "outputId": "c5a54ca0-4300-4576-eb8a-a7749655158e"
      },
      "outputs": [],
      "source": [
        "# Очищаем и лемматизируем текст\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "df[\"lemmatized_text\"] = df[\"clean_text\"].apply(lemmatized_text)\n",
        "# Удаляем из текста стоп-слова\n",
        "df[\"processed_text\"] = df[\"lemmatized_text\"].apply(del_stop_words)\n",
        "df[\"processed_text\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "4UZ_KQmVIioL",
        "outputId": "cdce88ac-867f-431f-d827-25bdfb4c0664"
      },
      "outputs": [],
      "source": [
        "# Разбиваем текст на шинглы\n",
        "df[\"shingles\"] = df[\"processed_text\"].apply(create_shingles, k=3)\n",
        "df[\"shingles\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "z31AcadpJv0-",
        "outputId": "049a8295-30c6-4ab4-8a88-b5fcb5c241d1"
      },
      "outputs": [],
      "source": [
        "# Создаем объект MinHash\n",
        "df[\"minhash\"] = df[\"shingles\"].apply(create_minhash, k=3)\n",
        "df[\"minhash\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuMISg-AQPjY",
        "outputId": "99bfe56b-8f97-4f55-d09a-736bf72c429d"
      },
      "outputs": [],
      "source": [
        "# Подсчитыаем LSH и находим потенциальные пары заявок\n",
        "results_lsh = calculate_lsh_similarity(df)\n",
        "results_lsh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdCljR-NTpXS",
        "outputId": "a6873278-d16f-4781-8ca5-0a35cfa2ce89"
      },
      "outputs": [],
      "source": [
        "# Находим похожие тексты, для которых сходство больше, чем trash 40%\n",
        "results_application = final_search_for_similar_requests(results_lsh, trash=40)\n",
        "results_application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrF1TMk5Yw1d",
        "outputId": "a3965312-4887-40ff-8cfa-f1fa2ced10e0"
      },
      "outputs": [],
      "source": [
        "# Выводим текст похожих новостей и информацию о заимствованиях\n",
        "for info in results_application:\n",
        "    ordinalnumber1, ordinalnumber2 = info[\"applications\"]\n",
        "    text1 = df[df[\"ordinalnumber\"] == ordinalnumber1][\"text\"].values[0]\n",
        "    text2 = df[df[\"ordinalnumber\"] == ordinalnumber2][\"text\"].values[0]\n",
        "    print(f\"Номер {ordinalnumber1}, оригинальный текст: {text1}\")\n",
        "    print(f\"Номер {ordinalnumber2}, текст с заимстованиями: {text2}\")\n",
        "    print(f\"Процент сходства: {info['similarity']}\")\n",
        "    print(f\"Совпадающие позиции слов: {info['shingles_positions']}\")\n",
        "    print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

# Анализ текстовых данных для выявления заимствований

Этот проект представляет собой решение для анализа текстовых данных и обнаружения заимствований на основе сравнения текстов из заданного набора. Основной функционал включает предварительную обработку текста, создание шинглов, вычисление MinHash и использование LSH для поиска похожих текстов.

## Описание шагов скрипта

1. **Предварительная обработка текста**:
   - Очистка текста от спецсимволов, чисел, пробелов и URL;
   - Лемматизация текста;
   - Удаление стоп-слов.

2. **Создание шинглов**:
   - Текст разбивается на последовательности из `k` слов.

3. **MinHash и LSH**:
   - Вычисление MinHash для каждого текста;
   - Поиск похожих текстов с использованием LSH (Locality-Sensitive Hashing).

4. **Сравнение текстов**:
   - Поиск пересечения шинглов между текстами;
   - Расчет процента совпадения на основе коэффициента Сёренсена;
   - Определение совпадающих позиций слов в тексте.

5. **Результаты**:
   - Для пар текстов с высоким процентом сходства (настраиваемый параметр `trash`), выводится информация о совпадениях, включая совпадающие позиции шинглов.

## Установка и запуск

### Зависимости

Для работы скрипта необходимы следующие библиотеки:

- `pandas`
- `nltk`
- `pymorphy3`
- `datasketch`
- `tqdm`

Установите зависимости с помощью `pip` и команды:

```bash
pip install - r requirements.txt
```

### Загрузка ресурсов NLTK

Скрипт требует загрузки токенизатора предложений Punkt и набора стоп-слов:

```python
nltk.download("punkt")
nltk.download("stopwords")
```

### Запуск

1. Подготовьте файл `applications_dataset.csv`, который расположен в репозитории, содержащий два столбца:
   - `ordinalnumber` — номер заявки;
   - `text` — текст заявки.

2. Запустите выполнение блокнота, чтобы обработать данные и получить пары текстов с заимствованиями.

## Ключевые функции

### Предварительная обработка текста

- **`clean_text`** — очистка текста от спецсимволов, чисел, пробелов и URL;
- **`lemmatized_text`** — лемматизация текста;
- **`del_stop_words`** — удаление стоп-слов.

### Обработка шинглов

- **`create_shingles`** — создание шинглов из текста;
- **`create_minhash`** — создание MinHash для набора шинглов;
- **`get_shingles_position`** — определение позиций шинглов в тексте;
- **`merge_positions`** — объединение перекрывающихся позиций шинглов.

### Сравнение текстов

- **`calculate_lsh_similarity`** — поиск потенциально похожих текстов с помощью LSH;
- **`final_search_for_similar_requests`** — расчет точного сходства между парами текстов и определение позиций совпадающих шинглов.

## Пример результатов

Для каждой пары текстов с совпадениями выводятся следующие данные:

- Номера заявок;
- Тексты оригинальной заявки и заявки с заимствованиями;
- Процент сходства;
- Совпадающие позиции слов.

### Пример вывода:
```
Номер 22-2-000000, оригинальный текст: "Текст первой заявки..."
Номер 22-2-000001, текст с заимствованиями: "Текст второй заявки..."
Процент сходства: 78.6%
Совпадающие позиции слов: [[0, 4], [10, 14]]
--------------------------------------------------------------------------------
```

## Настраиваемые параметры

- **`k`** — длина шинглов (по умолчанию 3);
- **`num_perm`** — количество хешей для MinHash (по умолчанию 128);
- **`threshold`** — порог схожести для LSH (по умолчанию 0.1);
- **`trash`** — минимальный процент совпадения для отображения результата (по умолчанию 40%).